"""
Gradient Boosting Tree model to predict win probability at 25 minutes.

Training features:
    • Frames 0–14: actual Total_Gold_Difference values.
    • Frames 15–24: forecasts generated by the hierarchical Seq2Seq gold model
      (`MultiStep_LSTM_DynamicWeight.py`).

Training / test splits mirror `visualize_by_performance.py`:
    • Matches are split with `train_test_split(..., test_size=0.2, random_state=42)`.
    • The 80% side is used for training; the held-out 20% is used for testing.
"""

import json
import sys
import warnings
from pathlib import Path
from typing import Dict, List, Sequence, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import torch
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    roc_auc_score,
    roc_curve,
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

warnings.filterwarnings("ignore")

# ----------------------------------------------------------------------------------------------------------------------
# Paths & constants
# ----------------------------------------------------------------------------------------------------------------------

ROOT = Path(__file__).resolve().parents[3]
sys.path.append(str(ROOT))

DATA_PATH = ROOT / "Data/processed/featured_data.parquet"
RESULTS_DIR = ROOT / "results/Model_GradientBoost_25Min"
RESULTS_DIR.mkdir(parents=True, exist_ok=True)

SEQ2SEQ_GOLD_PATH = Path("/Users/arjianma/ESE5380_FINAL/results/seq2seq_dynamic/seq2seq_dynamic_weight.pth")
MODEL_DEFINITION_PATH = ROOT / "src/models/Seq2Seq_MultiStep_LSTM/MultiStep_LSTM_DynamicWeight.py"

ENCODER_LENGTH = 15  # minutes 0-14
FORECAST_HORIZON = 10  # minutes 15-24
NUM_FEATURE_FRAMES = ENCODER_LENGTH + FORECAST_HORIZON

# ----------------------------------------------------------------------------------------------------------------------
# Import hierarchical seq2seq definitions dynamically
# ----------------------------------------------------------------------------------------------------------------------

import importlib.util

spec = importlib.util.spec_from_file_location("dynamic_seq2seq", MODEL_DEFINITION_PATH)
dynamic_module = importlib.util.module_from_spec(spec)
sys.modules[spec.name] = dynamic_module
spec.loader.exec_module(dynamic_module)

HierarchicalSeq2Seq = dynamic_module.HierarchicalSeq2Seq
TEAM_X1_FEATURES = dynamic_module.TEAM_X1_FEATURES
TEAM_X2_FEATURES = dynamic_module.TEAM_X2_FEATURES
PLAYER_FEATURE_ORDER = dynamic_module.PLAYER_FEATURE_ORDER
ALL_FEATURE_COLUMNS = dynamic_module.ALL_FEATURE_COLUMNS

# ----------------------------------------------------------------------------------------------------------------------
# Helper utilities
# ----------------------------------------------------------------------------------------------------------------------


def load_seq2seq_model(
    checkpoint_path: Path,
    target_column: str,
) -> Dict[str, object]:
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
    config = checkpoint.get("config", {})

    model = HierarchicalSeq2Seq(
        hidden_dim=config.get("hidden_dim", 128),
        proj_dim=config.get("proj_dim", 128),
        team_embed_dim=config.get("team_embed_dim", 32),
        teacher_forcing_ratio=0.0,
        condition_on_hidden=config.get("condition_on_hidden", True),
        horizon=FORECAST_HORIZON,
        recurrent_dropout=config.get("recurrent_dropout", 0.0),
    )
    model.load_state_dict(checkpoint["model_state_dict"])
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    feature_scaler = StandardScaler()
    feature_scaler.mean_ = np.array(checkpoint["scaler_mean"], dtype=np.float64)
    feature_scaler.scale_ = np.array(checkpoint["scaler_scale"], dtype=np.float64)
    feature_scaler.var_ = feature_scaler.scale_ ** 2
    feature_scaler.n_features_in_ = feature_scaler.mean_.shape[0]
    feature_scaler.n_samples_seen_ = 1

    target_scaler = StandardScaler()
    target_scaler.mean_ = np.array(checkpoint["target_scaler_mean"], dtype=np.float64)
    target_scaler.scale_ = np.array(checkpoint["target_scaler_scale"], dtype=np.float64)
    target_scaler.var_ = target_scaler.scale_ ** 2
    target_scaler.n_features_in_ = target_scaler.mean_.shape[0]
    target_scaler.n_samples_seen_ = 1

    feature_columns = checkpoint.get("feature_columns", ALL_FEATURE_COLUMNS)

    column_to_idx = {name: idx for idx, name in enumerate(feature_columns)}

    def ensure_columns(cols: Sequence[str], context: str) -> List[int]:
        missing = [c for c in cols if c not in column_to_idx]
        if missing:
            raise KeyError(f"Missing columns for {context}: {missing}")
        return [column_to_idx[c] for c in cols]

    x1_indices = ensure_columns(TEAM_X1_FEATURES, "TEAM_X1_FEATURES")
    x2_indices = ensure_columns(TEAM_X2_FEATURES, "TEAM_X2_FEATURES")
    player_indices = [
        ensure_columns([f"Player{i}_{feat}" for feat in PLAYER_FEATURE_ORDER], f"Player{i}")
        for i in range(1, 11)
    ]

    return {
        "model": model,
        "device": device,
        "feature_scaler": feature_scaler,
        "target_scaler": target_scaler,
        "feature_columns": feature_columns,
        "x1_indices": x1_indices,
        "x2_indices": x2_indices,
        "player_indices": player_indices,
        "target_column": target_column,
    }


def forecast_match(
    match_df: pd.DataFrame,
    model_info: Dict[str, object],
) -> np.ndarray:
    if len(match_df) < NUM_FEATURE_FRAMES:
        raise ValueError("Not enough frames for forecasting.")

    feature_columns = model_info["feature_columns"]
    features = match_df[feature_columns].values.astype(np.float32)
    scaled = model_info["feature_scaler"].transform(features)
    encoder_slice = scaled[:ENCODER_LENGTH]

    x1 = torch.from_numpy(encoder_slice[:, model_info["x1_indices"]]).unsqueeze(0)
    x2 = torch.from_numpy(encoder_slice[:, model_info["x2_indices"]]).unsqueeze(0)

    player_slices = [
        encoder_slice[:, idx_list] for idx_list in model_info["player_indices"]
    ]
    players = np.stack(player_slices, axis=1)
    players_tensor = torch.from_numpy(players).unsqueeze(0)

    target_series = match_df[model_info["target_column"]].values.astype(np.float32)
    if len(target_series) < ENCODER_LENGTH + FORECAST_HORIZON:
        raise ValueError("Target series shorter than required forecast horizon.")

    previous_raw = target_series[ENCODER_LENGTH - 1 : ENCODER_LENGTH - 1 + FORECAST_HORIZON]
    previous_scaled = model_info["target_scaler"].transform(previous_raw.reshape(-1, 1)).squeeze(-1)
    decoder_input = torch.from_numpy(previous_scaled).unsqueeze(0)

    with torch.no_grad():
        preds_scaled = model_info["model"](
            x1.to(model_info["device"]),
            x2.to(model_info["device"]),
            players_tensor.to(model_info["device"]),
            decoder_input.to(model_info["device"]),
            teacher_forcing_ratio=0.0,
        )

    preds_scaled = preds_scaled.squeeze(0).cpu().numpy()
    preds_raw = preds_scaled * model_info["target_scaler"].scale_[0] + model_info["target_scaler"].mean_[0]
    return preds_raw.astype(np.float32)


def get_match_label(match_df: pd.DataFrame) -> int:
    if "Y_won" in match_df.columns and not match_df["Y_won"].isna().all():
        value = match_df["Y_won"].iloc[0]
        if isinstance(value, bool):
            return int(value)
        if isinstance(value, (int, np.integer)):
            return 1 if value > 0 else 0
        try:
            as_float = float(value)
            return 1 if as_float > 0 else 0
        except Exception:
            value_str = str(value).lower()
            return 1 if value_str in {"win", "true", "1"} else 0
    final_gold = match_df["Total_Gold_Difference"].iloc[-1]
    return 1 if final_gold > 0 else 0


def build_dataset(
    match_ids: Sequence[str],
    dataframe: pd.DataFrame,
    gold_model: Dict[str, object],
    description: str,
    include_actual_full: bool = False,
) -> Tuple[np.ndarray, np.ndarray, List[str]]:
    features: List[np.ndarray] = []
    labels: List[int] = []
    kept_matches: List[str] = []

    for match_id in tqdm(match_ids, desc=f"Building {description} set"):
        match_df = (
            dataframe[dataframe["match_id"] == match_id]
            .copy()
            .sort_values("frame_idx")
            .reset_index(drop=True)
        )

        if len(match_df) < NUM_FEATURE_FRAMES:
            continue

        try:
            gold_forecast = forecast_match(match_df, gold_model)
        except Exception as exc:
            print(f"  Skipping match {match_id}: {exc}")
            continue

        gold_history = match_df["Total_Gold_Difference"].values[:ENCODER_LENGTH].astype(np.float32)

        if len(gold_history) < ENCODER_LENGTH:
            continue

        gold_series_forecast = np.concatenate([gold_history, gold_forecast])
        if gold_series_forecast.shape[0] != NUM_FEATURE_FRAMES:
            continue

        feature_vector_forecast = gold_series_forecast.astype(np.float32)
        label = get_match_label(match_df)

        features.append(feature_vector_forecast)
        labels.append(label)
        kept_matches.append(match_id)

        if include_actual_full:
            gold_actual = match_df["Total_Gold_Difference"].values[:NUM_FEATURE_FRAMES].astype(np.float32)
            if len(gold_actual) == NUM_FEATURE_FRAMES:
                feature_vector_actual = gold_actual.astype(np.float32)
                features.append(feature_vector_actual)
                labels.append(label)
                kept_matches.append(match_id)

    if not features:
        raise RuntimeError(f"No samples constructed for {description} set.")

    return np.stack(features), np.array(labels, dtype=int), kept_matches


def print_metrics(y_true, y_pred, y_proba, set_name: str) -> Tuple[float, float]:
    acc = accuracy_score(y_true, y_pred)
    try:
        auc = roc_auc_score(y_true, y_proba)
    except ValueError:
        auc = float("nan")

    print(f"\n{'=' * 60}")
    print(f"{set_name} METRICS")
    print(f"{'=' * 60}")
    print(f"Accuracy: {acc:.4f}")
    print(f"AUC-ROC: {auc:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=["Loss", "Win"]))
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_true, y_pred))
    return acc, auc


# ----------------------------------------------------------------------------------------------------------------------
# Main pipeline
# ----------------------------------------------------------------------------------------------------------------------

print("\n" + "=" * 60)
print("GRADIENT BOOSTING TREE - 25 MINUTE WIN PROBABILITY")
print("=" * 60)

print("\nLoading timeline data...")
df = pd.read_parquet(DATA_PATH)
df = df.sort_values(["match_id", "frame_idx"]).reset_index(drop=True)
print(f"✓ Data loaded: {df.shape}")

unique_matches = df["match_id"].unique()
train_ids, test_ids = train_test_split(
    unique_matches,
    test_size=0.2,
    random_state=42,
    shuffle=True,
)
train_matches = list(train_ids)
test_matches = list(test_ids)

print(f"\nMatch split (aligned with visualize_by_performance.py):")
print(f"  Train matches: {len(train_matches)}")
print(f"  Test matches : {len(test_matches)}")

print("\nLoading Seq2Seq gold model for forecasting...")
gold_model_info = load_seq2seq_model(SEQ2SEQ_GOLD_PATH, target_column="Total_Gold_Difference")
print("✓ Seq2Seq gold model ready.")

X_train, y_train, kept_train_matches = build_dataset(
    train_matches,
    df,
    gold_model_info,
    description="training",
    include_actual_full=True,
)
X_test, y_test, kept_test_matches = build_dataset(
    test_matches,
    df,
    gold_model_info,
    description="testing",
    include_actual_full=False,
)

print(f"\nConstructed datasets:")
print(f"  Train samples: {len(X_train)} (matches used: {len(kept_train_matches)})")
print(f"  Test samples : {len(X_test)} (matches used: {len(kept_test_matches)})")

feature_names = []
for idx in range(NUM_FEATURE_FRAMES):
    minute = idx
    source = "Actual" if idx < ENCODER_LENGTH else "Forecast"
    feature_names.append(f"Gold_Frame_{minute}_{source}")

# ----------------------------------------------------------------------------------------------------------------------
# Gradient Boosting
# ----------------------------------------------------------------------------------------------------------------------

n_estimators = 200
gb_model = GradientBoostingClassifier(
    n_estimators=n_estimators,
    learning_rate=0.1,
    max_depth=5,
    min_samples_split=2,
    min_samples_leaf=1,
    subsample=0.8,
    random_state=42,
    verbose=0,
    warm_start=True,
)

print("\n" + "=" * 60)
print("TRAINING GRADIENT BOOSTING CLASSIFIER")
print("=" * 60)
train_losses: List[Tuple[int, float]] = []
test_losses: List[Tuple[int, float]] = []

for i in tqdm(range(1, n_estimators + 1), desc="Training", unit="estimator"):
    gb_model.set_params(n_estimators=i)
    gb_model.fit(X_train, y_train)

    if i % 10 == 0 or i == n_estimators:
        train_pred = gb_model.predict(X_train)
        test_pred = gb_model.predict(X_test)
        train_acc = accuracy_score(y_train, train_pred)
        test_acc = accuracy_score(y_test, test_pred)
        train_losses.append((i, 1 - train_acc))
        test_losses.append((i, 1 - test_acc))

print("\n✓ Training complete!")

y_train_pred = gb_model.predict(X_train)
y_test_pred = gb_model.predict(X_test)
y_train_proba = gb_model.predict_proba(X_train)[:, 1]
y_test_proba = gb_model.predict_proba(X_test)[:, 1]

train_acc, train_auc = print_metrics(y_train, y_train_pred, y_train_proba, "TRAINING SET")
test_acc, test_auc = print_metrics(y_test, y_test_pred, y_test_proba, "TEST SET")

feature_importance = pd.DataFrame(
    {"feature": feature_names, "importance": gb_model.feature_importances_}
).sort_values("importance", ascending=False)

print(f"\n{'=' * 60}")
print("TOP 15 MOST IMPORTANT FEATURES")
print(f"{'=' * 60}")
print(feature_importance.head(15).to_string(index=False))

# ----------------------------------------------------------------------------------------------------------------------
# Visualizations
# ----------------------------------------------------------------------------------------------------------------------

fig = plt.figure(figsize=(18, 12))

plt.subplot(2, 3, 1)
train_iters, train_loss_vals = zip(*train_losses)
test_iters, test_loss_vals = zip(*test_losses)
plt.plot(train_iters, train_loss_vals, "b-", label="Train Error Rate", linewidth=2)
plt.plot(test_iters, test_loss_vals, "r-", label="Test Error Rate", linewidth=2)
plt.axhline(y=1 - test_acc, color="g", linestyle="--", linewidth=2, label=f"Final Test Error: {1 - test_acc:.4f}")
plt.xlabel("Number of Estimators", fontsize=12)
plt.ylabel("Error Rate", fontsize=12)
plt.title("Gradient Boosting - Error Rate Progression", fontsize=14, fontweight="bold")
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.text(
    0.02,
    0.98,
    f"Final Train Acc: {train_acc:.4f}\nFinal Test Acc: {test_acc:.4f}",
    transform=plt.gca().transAxes,
    fontsize=10,
    verticalalignment="top",
    bbox=dict(boxstyle="round", facecolor="wheat", alpha=0.5),
)

plt.subplot(2, 3, 2)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)
plt.plot(fpr_train, tpr_train, "b-", label=f"Train (AUC={train_auc:.3f})", linewidth=2)
plt.plot(fpr_test, tpr_test, "r-", label=f"Test (AUC={test_auc:.3f})", linewidth=2)
plt.plot([0, 1], [0, 1], "k--", label="Random", linewidth=1)
plt.xlabel("False Positive Rate", fontsize=12)
plt.ylabel("True Positive Rate", fontsize=12)
plt.title("ROC Curve", fontsize=14, fontweight="bold")
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(2, 3, 3)
cm = confusion_matrix(y_test, y_test_pred)
plt.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
plt.title("Test Set: Confusion Matrix", fontsize=14, fontweight="bold")
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ["Loss", "Win"])
plt.yticks(tick_marks, ["Loss", "Win"])
threshold = cm.max() / 2.0
for i, j in np.ndindex(cm.shape):
    plt.text(
        j,
        i,
        format(cm[i, j], "d"),
        horizontalalignment="center",
        color="white" if cm[i, j] > threshold else "black",
    )
plt.ylabel("True Label", fontsize=12)
plt.xlabel("Predicted Label", fontsize=12)

plt.subplot(2, 3, 4)
plt.hist(y_test_proba[y_test == 0], bins=20, alpha=0.5, label="Loss", color="red", edgecolor="black")
plt.hist(y_test_proba[y_test == 1], bins=20, alpha=0.5, label="Win", color="green", edgecolor="black")
plt.xlabel("Predicted Probability (Win)", fontsize=12)
plt.ylabel("Frequency", fontsize=12)
plt.title("Test Set: Probability Distribution", fontsize=14, fontweight="bold")
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(2, 3, 5)
class_acc = cm.diagonal() / cm.sum(axis=1)
classes = ["Loss", "Win"]
plt.bar(classes, class_acc, color=["red", "green"], alpha=0.7, edgecolor="black")
plt.ylabel("Accuracy", fontsize=12)
plt.title("Test Set: Accuracy by Class", fontsize=14, fontweight="bold")
plt.ylim([0, 1])
for idx, acc_value in enumerate(class_acc):
    plt.text(idx, acc_value + 0.02, f"{acc_value:.3f}", ha="center", fontsize=11, fontweight="bold")
plt.grid(True, alpha=0.3, axis="y")

plt.subplot(2, 3, 6)
top_features = feature_importance.head(15)
plt.barh(range(len(top_features)), top_features["importance"], color="steelblue", edgecolor="k")
plt.yticks(range(len(top_features)), top_features["feature"], fontsize=9)
plt.xlabel("Importance", fontsize=12)
plt.ylabel("Feature", fontsize=12)
plt.title("Top 15 Feature Importances", fontsize=14, fontweight="bold")
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis="x")

plt.tight_layout()
plt.savefig(RESULTS_DIR / "gradient_boosting_25min_results.png", dpi=300, bbox_inches="tight")
print(f"\n✓ Visualization saved as '{RESULTS_DIR / 'gradient_boosting_25min_results.png'}'")

plt.figure(figsize=(12, 10))
top_features = feature_importance.head(len(feature_importance))
plt.barh(range(len(top_features)), top_features["importance"], color="steelblue", edgecolor="k", linewidth=0.5)
plt.yticks(range(len(top_features)), top_features["feature"], fontsize=9)
plt.xlabel("Importance", fontsize=12)
plt.ylabel("Feature", fontsize=12)
plt.title(
    "All Feature Importances (Gold, Actual 0-14 + Forecast 15-24)",
    fontsize=14,
    fontweight="bold",
)
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3, axis="x")
plt.tight_layout()
plt.savefig(RESULTS_DIR / "gradient_boosting_25min_feature_importance.png", dpi=300, bbox_inches="tight")
print(f"✓ Feature importance plot saved as '{RESULTS_DIR / 'gradient_boosting_25min_feature_importance.png'}'")

feature_importance.to_csv(RESULTS_DIR / "feature_importance_25min.csv", index=False)
print(f"✓ Feature importance saved as '{RESULTS_DIR / 'feature_importance_25min.csv'}'")

summary = {
    "model": "Gradient Boosting Classifier",
    "n_estimators": n_estimators,
    "learning_rate": 0.1,
    "max_depth": 5,
    "features": "Gold differences (actual 0-14, Seq2Seq forecast 15-24)",
    "target": "Win/Loss",
    "train_accuracy": float(train_acc),
    "train_auc": float(train_auc),
    "test_accuracy": float(test_acc),
    "test_auc": float(test_auc),
    "train_samples": len(X_train),
    "test_samples": len(X_test),
}

with open(RESULTS_DIR / "model_summary.json", "w", encoding="utf-8") as f:
    json.dump(summary, f, indent=2)

print(f"\n✓ Model summary saved as '{RESULTS_DIR / 'model_summary.json'}'")

print(f"\n{'=' * 60}")
print("TRAINING COMPLETE")
print(f"{'=' * 60}")
print(f"\nModel predicts win/loss at 25 minutes using:")
print("  - Actual gold differences for minutes 0–14")
print("  - Seq2Seq gold forecasts for minutes 15–24")
print(f"\nTest Accuracy: {test_acc:.4f}")
print(f"Test AUC-ROC: {test_auc:.4f}")


