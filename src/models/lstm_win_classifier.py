#!/usr/bin/env python3
"""
Sequence-based win probability classifier using an LSTM encoder.

The model consumes historical feature sequences (optionally including gold-diff history)
and predicts the eventual match winner (`Y_won`) at different stages of the game.

- Variable-length sequences are handled via padding + pack_padded_sequence.
- Training data is generated by slicing each match at many timestamps, so the model learns
  to estimate win probability throughout the match progression.
- Checkpoints store the fitted scaler and feature list for reproducible inference.
"""

from __future__ import annotations

import argparse
import io
import math
import os
import pickle
from typing import List, Optional, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, roc_auc_score
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.dataloader import load_data_splits


def get_specified_features(csv_path: Optional[str] = None) -> List[str]:
    """
    Reuse the hard-coded feature selection from the LSTM regression pipeline.
    """
    if csv_path is not None:
        import pandas as pd

        if not os.path.exists(csv_path):
            print(f"‚ùå Warning: CSV file not found: {csv_path}")
        else:
            try:
                df = pd.read_csv(csv_path)
                if "feature" in df.columns:
                    col = "feature"
                elif "Feature" in df.columns:
                    col = "Feature"
                else:
                    col = df.columns[0]
                return df[col].dropna().tolist()
            except Exception as exc:
                print(f"‚ùå Error loading CSV ({csv_path}): {exc}")

    # Fallback: default set (includes scoreboard/gold diff history already engineered)
    return [
        "Total_Gold_Difference",
        "Total_Gold_Difference_Last_Time_Frame",
        "Total_Xp_Difference",
        "Total_Xp_Difference_Last_Time_Frame",
        "Total_Minions_Killed_Difference",
        "Total_Jungle_Minions_Killed_Difference",
        "Total_Kill_Difference",
        "Total_Assist_Difference",
        "Elite_Monster_Killed_Difference",
        "Buildings_Taken_Difference",
        "Magic_Damage_Done_Diff",
        "Magic_Damage_Done_To_Champions_Diff",
        "Magic_Damage_Taken_Diff",
        "Physical_Damage_Done_Diff",
        "Physical_Damage_Done_To_Champions_Diff",
        "Physical_Damage_Taken_Diff",
        "Total_Damage_Done_Diff",
        "Total_Damage_Done_To_Champions_Diff",
        "Total_Damage_Taken_Diff",
        "True_Damage_Done_Diff",
        "True_Damage_Done_To_Champions_Diff",
        "True_Damage_Taken_Diff",
        "Total_Ward_Placed_Difference",
        "Total_Ward_Killed_Difference",
        "Time_Enemy_Spent_Controlled_Difference",
        "CentroidDist",
        "MinInterTeamDist",
        "EngagedDiff",
        "FrontlineOverlap",
        "RadialVelocityDiff",
        "Blue_Team_Offensive_Score",
        "Blue_Team_Defensive_Score",
        "Blue_Team_Overall_Score",
        "Red_Team_Offensive_Score",
        "Red_Team_Defensive_Score",
        "Red_Team_Overall_Score",
        "Team_Offensive_Score_Diff",
        "Team_Defensive_Score_Diff",
        "Team_Overall_Score_Diff",
    ]


class WinRateSequenceDataset(Dataset):
    """
    Create variable-length sequences for win probability prediction.
    
    Two modes:
    1. use_prefix_data=False: Use full game sequence (one sequence per game)
    2. use_prefix_data=True: Create 3 sequences per game, ending at 50%, 75%, and 100% of game length
    """

    def __init__(
        self,
        data,
        feature_cols: List[str],
        target_col: str = "Y_won",
        max_sequence_length: int = 30,
        min_sequence_length: int = 5,
        scaler=None,
        fit_scaler: bool = True,
        use_prefix_data: bool = False,
        min_cutoff_ratio: float = 0.5,
        max_cutoff_ratio: float = 0.9,
        num_prefix_sequences: int = 3,
    ):
        self.feature_cols = feature_cols
        self.target_col = target_col
        self.max_sequence_length = max_sequence_length
        self.min_sequence_length = min_sequence_length
        self.scaler = scaler
        self.fit_scaler = fit_scaler
        self.use_prefix_data = use_prefix_data
        self.min_cutoff_ratio = min_cutoff_ratio
        self.max_cutoff_ratio = max_cutoff_ratio
        self.num_prefix_sequences = num_prefix_sequences

        (
            self.sequences,
            self.lengths,
            self.labels,
            self.match_ids,
            self.end_timestamps,
            self.end_frame_idxs,
        ) = self._prepare_sequences(data)

        if self.fit_scaler or self.scaler is not None:
            self._fit_scaler_if_needed()
            self._apply_scaling()

    def _prepare_sequences(self, data):
        sequences = []
        lengths = []
        labels = []
        match_ids = []
        end_timestamps = []
        end_frame_idxs = []

        required_cols = set(self.feature_cols + [self.target_col, "frame_idx"])
        missing = required_cols.difference(set(data.columns))
        if missing:
            raise ValueError(f"Dataset missing required columns: {missing}")

        # Ensure label is 0/1
        data = data.dropna(subset=[self.target_col])
        data[self.target_col] = data[self.target_col].astype(int)

        for match_id in data["match_id"].unique():
            match_df = (
                data[data["match_id"] == match_id]
                .sort_values("frame_idx")
                .reset_index(drop=True)
            )
            if match_df.empty:
                continue

            label = int(match_df[self.target_col].iloc[0])
            num_frames = len(match_df)

            if self.use_prefix_data:
                # Mode 2: Create multiple sequences per game with random cutoffs (data augmentation)
                # Generate random cutoff ratios for this game
                np.random.seed(hash(match_id) % (2**32))  # Deterministic randomness per game
                cutoff_ratios = np.random.uniform(
                    self.min_cutoff_ratio, 
                    self.max_cutoff_ratio, 
                    size=self.num_prefix_sequences
                )
                cutoff_ratios = np.sort(cutoff_ratios)  # Sort for consistency
                
                for ratio in cutoff_ratios:
                    end_idx = int(num_frames * ratio)
                    if end_idx < self.min_sequence_length:
                        continue
                    
                    # Use full sequence from start to end_idx (not limited by max_sequence_length)
                    seq_df = match_df.iloc[:end_idx]
                    seq_len = len(seq_df)

                    if seq_len < self.min_sequence_length:
                        continue

                    feature_values = seq_df[self.feature_cols].values.astype(np.float32)
                    end_timestamps.append(int(seq_df["timestamp"].iloc[-1]))
                    end_frame_idxs.append(int(seq_df["frame_idx"].iloc[-1]))
                    sequences.append(feature_values)
                    lengths.append(seq_len)
                    labels.append(label)
                    match_ids.append(match_id)
            else:
                # Mode 1: Use one sequence per game with random cutoff
                # Randomly cut the game to a ratio between min_cutoff_ratio and max_cutoff_ratio
                np.random.seed(hash(match_id) % (2**32))  # Deterministic randomness per game
                cutoff_ratio = np.random.uniform(self.min_cutoff_ratio, self.max_cutoff_ratio)
                end_idx = int(num_frames * cutoff_ratio)
                
                if end_idx < self.min_sequence_length:
                    continue
                
                seq_df = match_df.iloc[:end_idx]
                seq_len = len(seq_df)

                if seq_len < self.min_sequence_length:
                    continue

                feature_values = seq_df[self.feature_cols].values.astype(np.float32)
                end_timestamps.append(int(seq_df["timestamp"].iloc[-1]))
                end_frame_idxs.append(int(seq_df["frame_idx"].iloc[-1]))
                sequences.append(feature_values)
                lengths.append(seq_len)
                labels.append(label)
                match_ids.append(match_id)

        return sequences, lengths, labels, match_ids, end_timestamps, end_frame_idxs

    def _fit_scaler_if_needed(self):
        from sklearn.preprocessing import StandardScaler

        if self.scaler is None:
            self.scaler = StandardScaler()
            flat = np.concatenate(
                [seq for seq in self.sequences],
                axis=0,
            )
            self.scaler.fit(flat)

    def _apply_scaling(self):
        scaled_sequences = []
        for seq in self.sequences:
            scaled = self.scaler.transform(seq)
            scaled_sequences.append(scaled)
        self.sequences = scaled_sequences

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        sequence = torch.FloatTensor(self.sequences[idx])
        length = int(self.lengths[idx])
        label = torch.FloatTensor([self.labels[idx]])  # BCE expects float target
        match_id = self.match_ids[idx]

        return {
            "sequence": sequence,
            "length": torch.LongTensor([length]),
            "label": label,
            "match_id": match_id,
        }


def collate_fn(batch):
    sequences = [item["sequence"] for item in batch]
    lengths = [item["length"] for item in batch]
    labels = [item["label"] for item in batch]
    match_ids = [item["match_id"] for item in batch]

    padded_sequences = nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=0.0)
    lengths = torch.cat(lengths)
    labels = torch.cat(labels)

    return {
        "sequences": padded_sequences,
        "lengths": lengths,
        "labels": labels,
        "match_ids": match_ids,
    }


class WinRateLSTM(nn.Module):
    def __init__(
        self,
        input_size: int,
        hidden_size: int = 128,
        num_layers: int = 2,
        dropout: float = 0.3,
        bidirectional: bool = False,
    ):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0.0,
            batch_first=True,
            bidirectional=bidirectional,
        )
        lstm_out_dim = hidden_size * (2 if bidirectional else 1)
        self.head = nn.Sequential(
            nn.Linear(lstm_out_dim, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, 1),
        )

    def forward(self, sequences: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:
        packed = pack_padded_sequence(
            sequences, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        packed_out, _ = self.lstm(packed)
        unpacked, _ = pad_packed_sequence(packed_out, batch_first=True)

        batch_size = sequences.size(0)
        last_timesteps = lengths - 1
        last_hidden = unpacked[torch.arange(batch_size), last_timesteps, :]

        logits = self.head(last_hidden)
        return logits.squeeze(dim=-1)


def build_datasets(
    feature_list,
    max_sequence_length: int,
    min_sequence_length: int,
    use_prefix_data: bool = False,
    train_data_path: Optional[str] = None,
    val_data_path: Optional[str] = None,
    test_data_path: Optional[str] = None,
    min_cutoff_ratio: float = 0.5,
    max_cutoff_ratio: float = 0.9,
    num_prefix_sequences: int = 3,
):
    """
    Build datasets for training.
    
    Args:
        feature_list: List of features to use
        max_sequence_length: Maximum sequence length
        min_sequence_length: Minimum sequence length
        use_prefix_data: Whether to use prefix data mode
        train_data_path: Optional path to train parquet file (if None, uses default splits)
        val_data_path: Optional path to val parquet file (if None, uses default splits)
        test_data_path: Optional path to test parquet file (if None, uses default splits)
    """
    if train_data_path is not None or val_data_path is not None or test_data_path is not None:
        # Load from specified paths
        print("üìÇ Loading data from specified paths:")
        train_data = pd.read_parquet(train_data_path) if train_data_path else None
        if train_data_path:
            print(f"  Train: {train_data_path} ({len(train_data):,} rows)")
        val_data = pd.read_parquet(val_data_path) if val_data_path else None
        if val_data_path:
            print(f"  Val: {val_data_path} ({len(val_data):,} rows)")
        test_data = pd.read_parquet(test_data_path) if test_data_path else None
        if test_data_path:
            print(f"  Test: {test_data_path} ({len(test_data):,} rows)")
        
        # If any path is None, fall back to default splits for that split
        if train_data is None or val_data is None or test_data is None:
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            data_splits_path = os.path.join(project_root, "data", "splits")
            default_train, default_val, default_test = load_data_splits(data_splits_path)
            
            if train_data is None:
                train_data = default_train
                print(f"  Train: Using default from {data_splits_path} ({len(train_data):,} rows)")
            if val_data is None:
                val_data = default_val
                print(f"  Val: Using default from {data_splits_path} ({len(val_data):,} rows)")
            if test_data is None:
                test_data = default_test
                print(f"  Test: Using default from {data_splits_path} ({len(test_data):,} rows)")
    else:
        # Use default behavior
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        data_splits_path = os.path.join(project_root, "data", "splits")
        print(f"üìÇ Loading data from default splits: {data_splits_path}")
        train_data, val_data, test_data = load_data_splits(data_splits_path)
        print(f"  Train: {len(train_data):,} rows | Val: {len(val_data):,} rows | Test: {len(test_data):,} rows")

    y_col = "Y_won"
    # Metadata columns to exclude from features (including puuid, team for PCA data)
    metadata_cols = ["match_id", "frame_idx", "timestamp", "puuid", "team"]
    # Outcome variables to exclude from features (should not be used as input features)
    outcome_cols = [
        "Y_won",
        "Elite_Monster_Killed_Difference",
        "Buildings_Taken_Difference",
        "Total_Gold_Difference",
        "Total_Xp_Difference",
    ]

    if feature_list is not None:
        if isinstance(feature_list, str) and feature_list.endswith(".csv"):
            feature_list = get_specified_features(feature_list)
        elif feature_list == "specified":
            feature_list = get_specified_features()
        elif isinstance(feature_list, str):
            feature_list = [feature_list]

    if feature_list is None:
        # Exclude metadata and all outcome variables from features
        exclude_cols = metadata_cols + outcome_cols
        feature_cols = [
            col
            for col in train_data.columns
            if col not in exclude_cols
        ]
        print(f"Using all features ({len(feature_cols)}): {feature_cols[:10]}{'...' if len(feature_cols) > 10 else ''}")
    else:
        available = set(train_data.columns)
        # Exclude all outcome variables from feature list
        exclude_cols = set(outcome_cols)
        feature_cols = [feat for feat in feature_list if feat in available and feat not in exclude_cols]
        missing = [feat for feat in feature_list if feat not in available]
        if missing:
            print(f"‚ö† Missing {len(missing)} features from dataset: {missing[:5]}{'...' if len(missing) > 5 else ''}")

    train_dataset = WinRateSequenceDataset(
        train_data,
        feature_cols=feature_cols,
        target_col=y_col,
        max_sequence_length=max_sequence_length,
        min_sequence_length=min_sequence_length,
        scaler=None,
        fit_scaler=True,
        use_prefix_data=use_prefix_data,
        min_cutoff_ratio=min_cutoff_ratio,
        max_cutoff_ratio=max_cutoff_ratio,
        num_prefix_sequences=num_prefix_sequences,
    )

    val_dataset = WinRateSequenceDataset(
        val_data,
        feature_cols=feature_cols,
        target_col=y_col,
        max_sequence_length=max_sequence_length,
        min_sequence_length=min_sequence_length,
        scaler=train_dataset.scaler,
        fit_scaler=False,
        use_prefix_data=use_prefix_data,
        min_cutoff_ratio=min_cutoff_ratio,
        max_cutoff_ratio=max_cutoff_ratio,
        num_prefix_sequences=num_prefix_sequences,
    )

    test_dataset = WinRateSequenceDataset(
        test_data,
        feature_cols=feature_cols,
        target_col=y_col,
        max_sequence_length=max_sequence_length,
        min_sequence_length=min_sequence_length,
        scaler=train_dataset.scaler,
        fit_scaler=False,
        use_prefix_data=use_prefix_data,
        min_cutoff_ratio=min_cutoff_ratio,
        max_cutoff_ratio=max_cutoff_ratio,
        num_prefix_sequences=num_prefix_sequences,
    )

    return train_dataset, val_dataset, test_dataset, feature_cols, train_dataset.scaler


def build_dataloaders(train_dataset, val_dataset, test_dataset, batch_size: int, num_workers: int = 4):
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,
        drop_last=True,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,
    )
    return train_loader, val_loader, test_loader


def evaluate(model, data_loader, device):
    model.eval()
    criterion = nn.BCEWithLogitsLoss()
    losses = []
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for batch in data_loader:
            sequences = batch["sequences"].to(device)
            lengths = batch["lengths"].to(device)
            labels = batch["labels"].to(device).squeeze()

            logits = model(sequences, lengths)
            loss = criterion(logits, labels)
            losses.append(loss.item())

            probs = torch.sigmoid(logits).cpu().numpy()
            all_probs.append(probs)
            all_labels.append(labels.cpu().numpy())

    avg_loss = np.mean(losses) if losses else 0.0
    if all_probs:
        probs = np.concatenate(all_probs)
        labels = np.concatenate(all_labels)
        preds = (probs >= 0.5).astype(int)
        acc = accuracy_score(labels, preds)
        try:
            auc = roc_auc_score(labels, probs)
        except ValueError:
            auc = float("nan")
    else:
        acc = 0.0
        auc = float("nan")

    return avg_loss, acc, auc


def train_model(
    train_loader,
    val_loader,
    test_loader,
    input_size: int,
    args,
    feature_cols,
    scaler,
):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = WinRateLSTM(
        input_size=input_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        dropout=args.dropout,
        bidirectional=args.bidirectional,
    ).to(device)

    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
    criterion = nn.BCEWithLogitsLoss()

    best_val_loss = float("inf")
    best_state = None
    patience_counter = 0

    history = {
        "train_loss": [],
        "train_acc": [],
        "train_auc": [],
        "val_loss": [],
        "val_acc": [],
        "val_auc": [],
        "test_loss": [],
        "test_acc": [],
        "test_auc": [],
    }

    for epoch in range(1, args.num_epochs + 1):
        model.train()
        running_loss = 0.0
        train_probs_epoch = []
        train_labels_epoch = []

        for batch in tqdm(train_loader, desc=f"Epoch {epoch}/{args.num_epochs}", unit="batch", leave=False):
            sequences = batch["sequences"].to(device)
            lengths = batch["lengths"].to(device)
            labels = batch["labels"].to(device).squeeze()

            optimizer.zero_grad()
            logits = model(sequences, lengths)
            loss = criterion(logits, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            running_loss += loss.item()
            with torch.no_grad():
                probs_batch = torch.sigmoid(logits).detach().cpu().numpy()
                train_probs_epoch.append(probs_batch)
                train_labels_epoch.append(labels.detach().cpu().numpy())

        avg_train_loss = running_loss / len(train_loader)
        history["train_loss"].append(avg_train_loss)

        if train_probs_epoch:
            train_probs = np.concatenate(train_probs_epoch)
            train_labels = np.concatenate(train_labels_epoch)
            train_preds = (train_probs >= 0.5).astype(int)
            train_acc = accuracy_score(train_labels, train_preds)
            try:
                train_auc = roc_auc_score(train_labels, train_probs)
            except ValueError:
                train_auc = float("nan")
        else:
            train_acc = 0.0
            train_auc = float("nan")

        history["train_acc"].append(train_acc)
        history["train_auc"].append(train_auc)

        val_loss, val_acc, val_auc = evaluate(model, val_loader, device)
        test_loss, test_acc, test_auc = evaluate(model, test_loader, device)

        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)
        history["val_auc"].append(val_auc)
        history["test_loss"].append(test_loss)
        history["test_acc"].append(test_acc)
        history["test_auc"].append(test_auc)

        print(
            f"Epoch {epoch}/{args.num_epochs} "
            f"| Train Loss: {avg_train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f} "
            f"| Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, AUC: {val_auc:.4f} "
            f"| Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}"
        )

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            best_state = {
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "epoch": epoch,
                "val_loss": val_loss,
                "val_acc": val_acc,
                "val_auc": val_auc,
                "feature_cols": feature_cols,
                "input_size": input_size,
                "args": vars(args),
            }

            buffer = io.BytesIO()
            pickle.dump(scaler, buffer)
            best_state["feature_scaler"] = buffer.getvalue()

            save_path = os.path.join(args.model_save_dir, args.best_checkpoint_name)
            torch.save(best_state, save_path)
            print(f"  ‚úì Saved new best model to {save_path}")
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print("‚èπ Early stopping triggered.")
                break

    if best_state:
        final_path = os.path.join(args.model_save_dir, args.checkpoint_name)
        torch.save(best_state, final_path)
        print(f"‚úì Final checkpoint saved to {final_path}")

    return model, history


def plot_history(history, save_path: Optional[str] = None):
    import matplotlib.pyplot as plt

    epochs = range(1, len(history["train_loss"]) + 1)
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, history["train_loss"], label="Train Loss")
    plt.plot(epochs, history["val_loss"], label="Val Loss")
    plt.xlabel("Epoch")
    plt.ylabel("BCE Loss")
    plt.title("Training vs Validation Loss")
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(epochs, history["train_acc"], label="Train Accuracy")
    plt.plot(epochs, history["val_acc"], label="Val Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title("Training vs Validation Accuracy")
    plt.ylim(0, 1)
    plt.legend()
    plt.grid(True)

    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"‚úì Training curves saved to {save_path}")
    plt.close()


def parse_args():
    parser = argparse.ArgumentParser(
        description="Train an LSTM-based win probability classifier",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )

    parser.add_argument("--feature_list", type=str, default=None,
                        help='Feature list: CSV path, "specified", "None", or comma-separated list')
    parser.add_argument("--num_epochs", type=int, default=30)
    parser.add_argument("--patience", type=int, default=5)
    parser.add_argument("--batch_size", type=int, default=64)
    parser.add_argument("--learning_rate", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=0.0)
    parser.add_argument("--hidden_size", type=int, default=128)
    parser.add_argument("--num_layers", type=int, default=2)
    parser.add_argument("--dropout", type=float, default=0.3)
    parser.add_argument("--bidirectional", action="store_true")

    parser.add_argument("--max_sequence_length", type=int, default=40)
    parser.add_argument("--min_sequence_length", type=int, default=5)
    parser.add_argument("--use_prefix_data", action="store_true",
                        help="If set, create multiple sequences per game with random cutoffs (data augmentation). Otherwise, use one sequence per game with random cutoff.")
    parser.add_argument("--min_cutoff_ratio", type=float, default=0.5,
                        help="Minimum cutoff ratio for random cutoff (default: 0.5, i.e., 50%% of game length)")
    parser.add_argument("--max_cutoff_ratio", type=float, default=0.9,
                        help="Maximum cutoff ratio for random cutoff (default: 0.9, i.e., 90%% of game length)")
    parser.add_argument("--num_prefix_sequences", type=int, default=3,
                        help="Number of sequences to create per game in prefix mode (default: 3)")
    parser.add_argument("--train_data_path", type=str, default=None,
                        help="Path to train parquet file (if not specified, uses default data/splits/train.parquet)")
    parser.add_argument("--val_data_path", type=str, default=None,
                        help="Path to val parquet file (if not specified, uses default data/splits/val.parquet)")
    parser.add_argument("--test_data_path", type=str, default=None,
                        help="Path to test parquet file (if not specified, uses default data/splits/test.parquet)")

    parser.add_argument("--model_save_dir", type=str, default="models/win_classifier")
    parser.add_argument("--checkpoint_name", type=str, default="lstm_win_classifier.pth")
    parser.add_argument("--best_checkpoint_name", type=str, default="lstm_win_classifier_best.pth")
    parser.add_argument("--curve_save_path", type=str, default="results/Model_LSTM/win_classifier_training_curves.png")

    args = parser.parse_args()

    feature_arg = args.feature_list
    if feature_arg is not None:
        if feature_arg.lower() == "none":
            args.feature_list = None
        elif feature_arg.lower() == "specified":
            args.feature_list = "specified"
        elif "," in feature_arg and not feature_arg.endswith(".csv"):
            args.feature_list = [token.strip() for token in feature_arg.split(",")]

    return args


def main():
    args = parse_args()

    os.makedirs(args.model_save_dir, exist_ok=True)

    print("üöÄ Training LSTM Win Probability Classifier")
    print("=" * 60)
    
    if args.use_prefix_data:
        print(f"üìä Dataset mode: Prefix data ({args.num_prefix_sequences} sequences per game with random cutoffs)")
        print(f"   Cutoff range: {args.min_cutoff_ratio:.1%} - {args.max_cutoff_ratio:.1%} of game length")
    else:
        print("üìä Dataset mode: Full game sequences (1 sequence per game with random cutoff)")
        print(f"   Cutoff range: {args.min_cutoff_ratio:.1%} - {args.max_cutoff_ratio:.1%} of game length")

    train_dataset, val_dataset, test_dataset, feature_cols, scaler = build_datasets(
        feature_list=args.feature_list,
        max_sequence_length=args.max_sequence_length,
        min_sequence_length=args.min_sequence_length,
        use_prefix_data=args.use_prefix_data,
        train_data_path=args.train_data_path,
        val_data_path=args.val_data_path,
        test_data_path=args.test_data_path,
        min_cutoff_ratio=args.min_cutoff_ratio,
        max_cutoff_ratio=args.max_cutoff_ratio,
        num_prefix_sequences=args.num_prefix_sequences,
    )

    train_loader, val_loader, test_loader = build_dataloaders(
        train_dataset, val_dataset, test_dataset, batch_size=args.batch_size
    )

    input_size = train_dataset.sequences[0].shape[-1]
    print(f"Feature count: {input_size}")
    print(f"Train sequences: {len(train_dataset)} | Val sequences: {len(val_dataset)} | Test sequences: {len(test_dataset)}")

    model, history = train_model(
        train_loader=train_loader,
        val_loader=val_loader,
        test_loader=test_loader,
        input_size=input_size,
        args=args,
        feature_cols=feature_cols,
        scaler=scaler,
    )

    if history["train_loss"]:
        plot_history(history, save_path=args.curve_save_path)

    print("\n‚úÖ Training complete!")
    print("=" * 60)


if __name__ == "__main__":
    main()

